# 机器学习是什么？  

## 个人理解  

### 解密机器学习  

人工智能（AI）、机器学习、大数据分析，很多近年听到的新名词。机器学习是AI的一个分支，区别于传统的方法由专家设计好算法来让机器获得“智能”，机器学习是通过已知数据来学习规律，而各种规律用函数来表示，机器学习就变成了寻找函数的最佳参数的过程。通过机器学习出来的成果可以用来进行大数据分析，有时候机器学习的过程也是对大数据的一个分析过程，特别是无监督学习。  

机器学习的本质就是学习一个函数$f(x)$，使输入的信息通过$f(x)$得到需要的结果。如：线性回归就是得到一个具体的数字，如逻辑回归则得到属于哪一类的可能性。  

### 机器学习在干什么  

机器学习的过程其实就是求解$f(\vec{X}) = \vec{W}\vec{X}$ 的函数参数 $\vec{W}$ 的过程。  

举个例子：  
对于线性回归，假设 $f(x)=wx+b$ ，则此函数中$x$是未知数，也就是输入信息（也叫特征），当$w$和$b$确定的情况下，根据不同的 $x$ 能求解出对应的 $f(x)$ 。  
机器学习是根据已知的有限数据，反推出$w$和$b$的值，然后确定此函数。机器学习的过程就是求得 $w$ 和 $b$ ，使 $f(x)$ 取得最好效果的过程。  
在此对$w$和$b$统称为待求解的参数。

### 机器怎样学习  

既然机器学习是求解参数，那么怎么求解参数呢？  

#### 正则方程  

对于简单的线性方程，就如一个二元一次方程，我们有两组数据（包括输入 $x$ 和结果 $f(x)$ ），就可以求解出来 $w$ 和 $b$ 。  
如 $(1)$ 所示方程组：  
$$
\left\{
    \begin{array}{lcl}
    y_1 & = & wx_1 + b  \tag{1}\\
    y_2 & = & wx_2 + b \\
    \end{array}
\right.
$$  
由 $(1)$ 可以轻松求得它的解。  
$$
\left\{
    \begin{array}{lcl}
    \Large w = \frac{y_1-y_2}{x_1-x_2}\\
    \Large b = \frac{x_1y_2-x_2y_1}{x_1-x_2}
    \end{array}
\right.
$$

#### 损失函数  

损失函数就是使用函数$f(x)$预测的值$\hat{y}$与真实值$y$的距离进行比较。如线性回归一般使用均方差 $loss=\frac{1}{2m} \sum_{i=1}^{m}(\hat{y^{(i)}} - y^{(i)})^2$ ，可以视为求两者的欧式距离的平方再求均值。至于除以2，是为了求导以后系数为1。  

#### 梯度下降  

机器学习就是要求 $\vec{W}$ ，使 $f(\vec{X}) = \vec{W}\vec{X}$ 预测出来的值 $\hat{y}$ 与真实值 $y$ 距离尽可能的小，理想状态是为0。  

也就等价于让损失 $loss$ 的值尽可能的小。就是要对损失函数 $J(\vec{w}, b)$ 求出最小值，当然实际情况除了凸函数外很难找到它的最小值，那么我们转而取它的极小值。通过随机化初始化参数等方法尽可能找到极小值中的最小一个极小值。  

而为了求得最小值， $J(\vec{w}, b)$ 是关于$w$的函数，那么根据 $w$ 当前的值所在位置的导数，朝最低点的方向都是导数的负数方向，因此不断的向 $w$ 所在点的导数的负方向，就可以到达loss的极小值点。这个过程就叫梯度下降。

为了让 $w$ 下降的步伐控制在合理范围内，加入了参数 $\alpha$ ，因些得到梯度下降公式 $w_n^{(i)} = w_{n-1}^{(i)} - \alpha \times \frac{\partial{}}{\partial{w_i}} J(\vec{w}, b)$ ，其中 $n$ 代表第n次梯度下降， $n \ge 2$ ， $w_1$ 为权重初始化值。

函数  

线性回归  

$$
f(\vec{w}, b) = \vec{w} \cdot \vec{x} + b
$$

逻辑回归  

$$
f(\vec{w}, b) = \Large\frac{1}{1+e^{(-\vec{w}\cdot\vec{x}+b)}}
$$

常用损失函数  

均方差（MSE）损失函数
$$J(\vec{w}, b)=\frac{1}{2m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2$$  

加入正则项  
$$J(\vec{w}, b)=\frac{1}{2m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2 \color{blue}+ \frac{\lambda}{2m}\sum_{j=1}^{n}w_j^2$$  

二元交叉熵（BCE）损失函数  
$$
J(\vec{w}, b) = -\frac{1}{m} \sum_{i=1}^{m} \left [y^{(i)}\log{\left(\hat{y}^{(i)}\right)} - (1 - y^{(i)})\log{\left(1-\hat{y}^{(i)}\right)} \right ]
$$

交叉熵（CE）损失函数  
$$
J(\vec{w}, b) = -\frac{1}{m} \sum_{i=1}^{m} \sum_{c=1}^{M} \left [y_{ic}\log{\left(p_{ic}\right)} \right ]
$$  
$m$ 是样本数量， $M$ 是类别数量。
$y_{ic}$ 真实样本类别。
$p_{ic}$ 观测样本 $i$ 属于类别 $c$ 的预测概率  

求导  
$$\frac{\partial}{\partial{w_j}}J(\vec{w}, b) = \frac{1}{m} \sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)} - y^{(i)})x_j^{(i)}$$  
$$\frac{\partial}{\partial{b}}J(\vec{w}, b) = \frac{1}{m} \sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)} - y^{(i)})$$

带正则项求导  
$$\frac{\partial}{\partial{w_j}}J(\vec{w}, b) = \frac{1}{m} \sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)} - y^{(i)})x_j^{(i)} \color{blue}+ \frac{\lambda}{m}w_j$$  
$$\frac{\partial}{\partial{b}}J(\vec{w}, b) = \frac{1}{m} \sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)} - y^{(i)})$$

梯度下降  
repeat {
$$
\left\{
    \begin{array}{lcl}
    \Large w_j = w_j - \alpha \frac{\partial}{\partial{w_j}}J(\vec{w}, b)\\
    \Large b = b - \alpha \frac{\partial}{\partial{b}}J(\vec{w}, b)
    \end{array}
\right.
$$
}  

#### 批量随机梯度下降  

#### 归一化  

#### 线性回归  

线性回归不一定只是直线，有可能是一条曲线。线性回归的本质是通过 $f(x)$ 来预测一个数值，这个值是根据公式算出来的任意值，而不是固定的0、1，或者指定的类别。

#### 逻辑回归  

逻辑判断一般用true、false来表达，逻辑回归其实就是类似意思，对结果进行是与否的判断。  
逻辑回归的二分类进行扩展后就可以实现多类分类以及多标签分类，所以实现多类分类以及多标签分类一般仍似为逻辑回归序列。  
